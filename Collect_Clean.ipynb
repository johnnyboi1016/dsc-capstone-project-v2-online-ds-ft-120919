{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flatiron School for Data Science - Capstone Project\n",
    "* Student name: John J. Cho\n",
    "* Student pace: full time online\n",
    "* Instructor name: Rafael Carrasco\n",
    "* Blog post URL: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effects of COVID-19 Shelter-In-Place on Nintendo Switch Resale Pricing\n",
    ">### In this project, we will be exploring the imbalance in supply and demand for the Nintendo Switch primarily reflected in closing prices on eBay, as well as other retailers.\n",
    "### Introduction:\n",
    "The **Nintendo Switch** was released in March 2017 at a retail price of $299.99 as a successor to the Wii U, which had ended production 2 months prior. As Nintendo's 8th generation gaming console, the Switch competes with Microsoft's Xbox One and Sony's PlayStation 4 - although many experts will agree that it has carved out a niche market of its own. This is due to their ecosystem having much more universal appeal, with a wide focus on family friendly games for all ages and both genders. The dual delivery platform (physical cartridge / online download) and growing variety of games at every price point has ushered in a 'second wave' of growth for 2020, exacerbated by the **worldwide coronavirus pandemic.**\n",
    "\n",
    "In August 2019, the original Switch was updated with a newer processor, translating into improved battery life. A month later, Nintendo released a handheld-only version of the console, the **Switch Lite** - at a retail price of $199.99. Merely 10 days after launch, the Lite had sold almost 2M units. Reported 2020 Q1 earnings revealed over 6M units sold for the Lite and almost 50M units for the original Switch. It was apparent that the Switch was **getting its second wind** when March sales doubled from 2019 to 2020. In addition, one of its most anticipated games - **Animal Crossing: New Horizons** - was released on March 20. Per [The Verge](https://www.theverge.com/2020/3/26/21195022/animal-crossing-switch-sales-japan-famitsu), almost 2M physical copies (excluding download purchases!) were sold in ***just the first 3 opening days in Japan alone.*** Almost 400k Switch systems were also sold in Japan the week following the game's release, setting a new record.\n",
    "\n",
    "**Beginning on March 16**, the state of California mandated a stay-at-home / shelter-in-place policy to prevent medical facilities from being overwhelmed by the spread of COVID-19. Many states (but not all) put forth similar mandates in the following weeks, shuttering all but essential businesses across the US and requiring residents to stay at home and follow social distancing rules when in public. Panicked buying of emergency supplies ensued - notably affecting toilet paper, hand sanitizer and face masks. Some opportunists hoarded these supplies and attempted to resell these items for large profits, thwarted by laws preventing this type of predatory behavior.\n",
    "\n",
    "However, these laws do not extend to non-essential products and many Americans realized that now would be a great time to have a Nintendo Switch at home to help pass the time (and keep the kids occupied since schools were shut down). This surge in demand ran headlong into a drop in supply as the worldwide pandemic started taking hold, affecting production at factories in Asia. Unnamed factory sources claimed that April shipments would be primarily affected - **as simultaneously the pool of interested Switch buyers started increasing exponentially.**\n",
    "\n",
    ">**A perfect economic storm was brewing** - the effects of which were captured by Switch closing prices on eBay.\n",
    "\n",
    "<img src=\"supply_demand.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Econ 101 informs us that price equilibrium occurs when market supply appropriately matches demand (Price=P1 given Supply=S at Quantity=Q1 given Demand=D). If supply were to suddenly reduce (S-->S2), the equilibrium price would increase. The same goes for a sudden demand increase (D-->D2 - FOMO being the #1 reason these days).\n",
    ">The timing of a flagship title release, millions of people forced to stay home and disrupted supply lines exacerbated by the increasing use of [online purchasing bots](https://www.washingtonpost.com/local/meet-the-virginia-teen-who-created-bird-bot-fueling-nintendo-switch-hoarders/2020/04/27/345919fe-889f-11ea-ac8a-fe9b8088e101_story.html) by resellers - all contributed to **a market with sellers eager to maximize profits and buyers stretching their budgets for how badly they wanted one.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection and Cleaning\n",
    "* Despite the obvious inefficiency in manually coding for the scraping of eBay listings using Beautiful Soup rather than using their API, I desired to gain the experience (and pain :-] ) in doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T03:43:57.298822Z",
     "start_time": "2020-05-14T03:43:54.151927Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime as dt\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T22:57:59.727589Z",
     "start_time": "2020-05-14T22:57:59.693556Z"
    }
   },
   "outputs": [],
   "source": [
    "# eBay scraper function\n",
    "def scrape_ebay(url,pages):\n",
    "    \"\"\"\n",
    "    Function to scrape auction data from eBay using Beautiful Soup.\n",
    "    The URL link and number of pages (set to show maximum # of listings - 200) must be defined and a dataframe will be returned\n",
    "    with populated values for Title, Date, Price, Shipping Cost, Unit Condition and Link to the auction URL.\n",
    "    \"\"\"\n",
    "    titles,dates,prices,shipping,condition,subtitles,links,sellers = [], [], [], [], [], [], [], []\n",
    "    for page in range(1,pages+1):  #loop thru every page of max results (200)\n",
    "        \n",
    "        # Get html page data using BeautifulSoup\n",
    "        req = requests.get(url.format(page))\n",
    "        soup = BeautifulSoup(req.content, 'html.parser')\n",
    "        listings = soup.find_all('li', attrs={'class': 's-item'})\n",
    "\n",
    "        for listing in listings:  #loop thru each listing on page\n",
    "            for name in listing.find_all('h3', attrs={'class':\"s-item__title\"}):  #filter for errors if listing is invalid\n",
    "                if (str(name.find(text=True, recursive=False))!='None') and (listing!=listings[0]):  #listing[0] is page headers\n",
    "                    \n",
    "                    # Get auction title\n",
    "                    name = listing.find('h3', attrs={'class':\"s-item__title\"})\n",
    "                    title = str(name.find(text=True, recursive=False))\n",
    "                    titles.append(title)\n",
    "                    \n",
    "                    # Get ending date\n",
    "                    date = listing.find('span', attrs={'class':\"s-item__ended-date\"})\n",
    "                    date = date.find(text=True)\n",
    "                    dates.append(date)\n",
    "\n",
    "                    # Get ending price\n",
    "                    price = listing.find('span', attrs={'class':\"s-item__price\"})\n",
    "                    price = price.find(text=True)\n",
    "                    price = price.replace('$','')\n",
    "                    price = price.replace(',','')\n",
    "                    prices.append(price)\n",
    "\n",
    "                    # Get shipping info\n",
    "                    ship = listing.find('span', attrs={'class':\"s-item__shipping\"})\n",
    "                    if ship==None: ship = listing.find_all('span', attrs={'class':\"s-item__shipping\"})\n",
    "                    else: ship = ship.find(text=True)\n",
    "                    shipping.append(ship)\n",
    "\n",
    "                    # Get condition info\n",
    "                    cond = listing.find('div', attrs={'class':\"s-item__subtitle\"})\n",
    "                    #cond2 = cond.find(attrs={'class':\"SECONDARY_INFO\"})\n",
    "                    cond = cond.find(text=True)\n",
    "                    condition.append(cond)\n",
    "\n",
    "                    # Get auction link to scrape more info\n",
    "                    link = listing.find_all(\"a\", attrs={'class':'s-item__link'})\n",
    "                    links.append(str(link))\n",
    "                    \n",
    "    # Put all info into dataframe\n",
    "    scraped = pd.DataFrame({\"Date\":dates,\"Title\":titles,\"Price\":prices,\"Shipping\":shipping,\"Condition\":condition,\n",
    "                            \"Link\":links})\n",
    "    scraped.Link = scraped.Link.apply(lambda x: x.split('href=\"')[1].split('\"')[0])  #clean up html link\n",
    "    \n",
    "    # Add year 2020 in front and convert all date values to datetime object\n",
    "    scraped.Date = scraped.Date.apply(lambda x: '2020-'+x)\n",
    "    scraped.Date = pd.to_datetime(scraped.Date)\n",
    "    scraped = scraped.sort_values(by=['Date'], ascending=False)\n",
    "    return scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T20:24:36.000256Z",
     "start_time": "2020-05-13T20:21:22.532384Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filtering for Nintendo Switch, Sold Items, US/Canada, max (200) listings per page, Gray color only\n",
    "pages = 7  #have to manually check then set how many pages to scrape\n",
    "url = \"https://www.ebay.com/sch/i.html?_oaa=1&_dcat=139971&Model=Nintendo%2520Switch&Color=Gray&_fsrp=1&_nkw=nintendo+switch&LH_Complete=1&rt=nc&Region%2520Code=NTSC%252DU%252FC%2520%2528US%252FCanada%2529&LH_Sold=1&_ipg=200&_pgn={}\"\n",
    "df_gray = scrape_ebay(url,pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for other colors (Black, Blue, White, Clear, Green, Multicolor, Orange, Pink, Purple, Red, Yellow, Not Specified)\n",
    "pages = 4\n",
    "url = \"https://www.ebay.com/sch/i.html?_oaa=1&_dcat=139971&Model=Nintendo%2520Switch&Color=Clear%7CGreen%7CMulticolor%7COrange%7CPink%7CPurple%7CRed%7CYellow%7C%21%7CBlack%7CBlue%7CWhite&_fsrp=1&_nkw=nintendo+switch&LH_Complete=1&Region%2520Code=NTSC%252DU%252FC%2520%2528US%252FCanada%2529&LH_Sold=1&_ipg=200&_pgn={}&rt=nc\"\n",
    "df_misc = scrape_ebay(url,pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for Switch Lite only\n",
    "pages = 6\n",
    "url = \"https://www.ebay.com/sch/i.html?_oaa=1&_dcat=139971&Model=Nintendo%2520Switch%2520Lite&_fsrp=1&_nkw=nintendo+switch&LH_Complete=1&Region%2520Code=NTSC%252DU%252FC%2520%2528US%252FCanada%2529&LH_Sold=1&_ipg=200&_pgn={}&rt=nc\"\n",
    "df_lite = scrape_ebay(url,pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T22:54:59.359504Z",
     "start_time": "2020-05-14T22:54:07.063154Z"
    }
   },
   "outputs": [],
   "source": [
    "pages = 76\n",
    "url = \"https://www.ebay.com/sch/i.html?_oaa=1&_dcat=139971&Model=Nintendo%2520Switch&_fsrp=1&_nkw=nintendo+switch&LH_Complete=1&Region%2520Code=NTSC%252DU%252FC%2520%2528US%252FCanada%2529&LH_Sold=1&_ipg=200&_pgn={}&rt=nc\"\n",
    "df1 = scrape_ebay(url,pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T00:06:20.465708Z",
     "start_time": "2020-05-14T23:59:00.383238Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n",
      "159\n",
      "160\n",
      "163\n",
      "164\n",
      "168\n",
      "169\n",
      "171\n",
      "172\n",
      "174\n",
      "176\n",
      "177\n",
      "178\n",
      "180\n",
      "183\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "193\n",
      "196\n",
      "197\n",
      "199\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "208\n",
      "209\n",
      "212\n",
      "213\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "220\n",
      "223\n",
      "224\n",
      "225\n",
      "227\n",
      "230\n"
     ]
    }
   ],
   "source": [
    "# Get additional details: item location, seller username, feedback count and score using direct page link\n",
    "# Not defined as a function so variables can persist during troubleshooting and tendency for web pages to change/disappear\n",
    "#i = 0  #index counter\n",
    "#location, feedback, seller, fbscore = [], [], [], []\n",
    "\n",
    "for url in df.Link.iloc[i:]:\n",
    "\n",
    "    # Pull additional details from link to auction details\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    listing = soup.find_all('tr', attrs={'class': 'vi-ht20'})\n",
    "        \n",
    "    if listing==[]:  #filtering for older listings that need to grab another link to get to correct listing page\n",
    "        listing = soup.find('div', attrs={'class': 'nodestar-item-card-details__view'})\n",
    "        if listing==None:  #filtering for power seller with active store\n",
    "            listing = soup.find_all('tr', attrs={'class': 'vi-ht20'})\n",
    "        else:\n",
    "            link = listing.a.get('href')\n",
    "            df.Link.iloc[i] = link\n",
    "            req = requests.get(link)\n",
    "            soup = BeautifulSoup(req.content, 'html.parser')\n",
    "            listing = soup.find_all('tr', attrs={'class': 'vi-ht20'})\n",
    "    \n",
    "    if listing!=[]:  #filter for different page layouts\n",
    "            \n",
    "        # Get item location\n",
    "        temp = listing[3].find_all('div', attrs={'class': 'u-flL'})\n",
    "        loc = temp[1].find(text=True)\n",
    "        if loc=='\\n':  #slightly diff page layout\n",
    "            loc = listing[4].find_all('div', attrs={'class': 'u-flL'})[1].get_text()\n",
    "        location.append(loc)\n",
    "        # Get feedback count\n",
    "        try: feed = listing[4].find_all('a')[1].get_text()  #different page layouts\n",
    "        except: feed = listing[5].find('div', attrs={'class': 'mbg vi-VR-margBtm3'}).find_all('a')[1].get_text()\n",
    "        feedback.append(feed)\n",
    "        # Get seller username\n",
    "        try: name = listing[4].find('div', attrs={'class': 'mbg vi-VR-margBtm3'}).a.get_text()  #different page layouts\n",
    "        except: name = listing[5].find('div', attrs={'class': 'mbg vi-VR-margBtm3'}).a.get_text()\n",
    "        seller.append(name)\n",
    "        # Get feedback score (need to navigate to seller's page first)\n",
    "        try: link = listing[4].find('div', attrs={'class':'mbg vi-VR-margBtm3'}).a.get('href')  #different page layouts\n",
    "        except: link = listing[5].find('div', attrs={'class': 'mbg vi-VR-margBtm3'}).a.get('href')\n",
    "        req = requests.get(link)\n",
    "        soup = BeautifulSoup(req.content, 'html.parser')\n",
    "        listing = soup.find('div', attrs={'class':'perctg'})\n",
    "        if listing.get_text()=='':  #filter for no feedback in last 12 months which does not report %\n",
    "            score = 100\n",
    "        else:\n",
    "            score = listing.get_text().split('%')[0].split('\\t')[2]\n",
    "        fbscore.append(score)\n",
    "        \n",
    "    elif soup.find_all(text='We looked everywhere.') != []:  #filter for dead pages (drop row)\n",
    "        print(i,'Dead Page!')\n",
    "        df.drop(df.iloc[i:i+1].index, inplace=True)\n",
    "        i-=1        \n",
    "    else:\n",
    "        print(i)\n",
    "\n",
    "        # Get item location\n",
    "        listing = soup.find_all('div', attrs={'class': 'iti-eu-bld-gry'})\n",
    "        if listing==[]:  #filter for pages requiring another link to listing\n",
    "            listing = soup.find_all('div', attrs={'class': 'nodestar-item-card-details__view'})\n",
    "            temp = soup.find_all('div', attrs={'class': 'app-status-messages'})[0].get_text()\n",
    "            if listing==[] and temp==\"We're sorry, something went wrong. Please try again.\":  #filter for dead listing page\n",
    "                print(i,'Dead Page!!!')\n",
    "                break#df.drop(df.iloc[i:i+1].index, inplace=True)\n",
    "            link = listing[0].a.get('href')\n",
    "            req = requests.get(link)\n",
    "            soup = BeautifulSoup(req.content, 'html.parser')\n",
    "            listing = soup.find_all('div', attrs={'class': 'iti-eu-bld-gry'})\n",
    "        loc = listing[0].find().get_text()\n",
    "        location.append(loc)\n",
    "        # Get seller username\n",
    "        listing = soup.find('div', attrs={'class': 'mbg vi-VR-margBtm3'})\n",
    "        name = listing.a.get_text()\n",
    "        seller.append(name)\n",
    "        # Get feedback count\n",
    "        feed = listing.find_all('a')[1].get_text()\n",
    "        feedback.append(feed)\n",
    "        # Get feedback score\n",
    "        listing = soup.find('div', attrs={'id':'si-fb'})\n",
    "        if listing!=None:  #filter for pages with score displayed otherwise need to navigate to seller's page\n",
    "            score = listing.get_text().split('%')[0]\n",
    "        elif feed=='0':  #filter for new accounts (zero feedback) since page won't list %\n",
    "            score = 100\n",
    "        elif listing==None:  #navigate to seller's page\n",
    "            link = soup.find('div', attrs={'class':'mbg vi-VR-margBtm3'}).a.get('href')\n",
    "            req = requests.get(link)\n",
    "            soup = BeautifulSoup(req.content, 'html.parser')\n",
    "            listing = soup.find('div', attrs={'class':'perctg'})\n",
    "            if listing.get_text()=='':  #filter for no feedback in last 12 months which does not report %\n",
    "                score = 100\n",
    "            else:\n",
    "                score = listing.get_text().split('%')[0].split('\\t')[2]\n",
    "        else: \n",
    "            print(i,'!!!')\n",
    "            break\n",
    "        fbscore.append(score)\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T00:07:24.670205Z",
     "start_time": "2020-05-15T00:07:24.658490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232 232 232 232 232\n",
      "Waipahu, Hawaii, United States  edolfin 935 100\n"
     ]
    }
   ],
   "source": [
    "# Troubleshooting commands\n",
    "print(len(location),len(seller),len(feedback),len(fbscore), i)\n",
    "print(location[-1], seller[-1], feedback[-1], fbscore[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T05:24:10.854029Z",
     "start_time": "2020-05-14T05:24:10.845514Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(location[230], seller[230], feedback[230], fbscore[230])\n",
    "#print(location[264], seller[264], feedback[264], fbscore[264])\n",
    "#print(location[-1], seller[-1], feedback[-1], fbscore[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T23:57:09.325114Z",
     "start_time": "2020-05-14T23:57:09.315908Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Link.iloc[156] == url\n",
    "#df.Link.iloc[890] = 'https://www.ebay.com/itm/333594204297?nordt=true&orig_cvip=true&rt=nc&_trksid=p2546137.m43663.l10137'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T23:57:14.537256Z",
     "start_time": "2020-05-14T23:57:14.513943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.ebay.com/itm/NEW-Animal-Crossing-New-Horizon-Special-Edition-Nintendo-Switch-Console-PREORDER/114118799150?epid=28036457807&amp;hash=item1a9202af2e:g:C2IAAOSwEdteSj62'"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T23:58:22.799892Z",
     "start_time": "2020-05-14T23:58:22.791270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.ebay.com/itm/NEW-Animal-Crossing-New-Horizon-Special-Edition-Nintendo-Switch-Console-PREORDER-/114118799150?_trksid=p2349526.m4383.l10137.c10&nordt=true&rt=nc&orig_cvip=true'"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Link.iloc[156]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T23:42:20.390584Z",
     "start_time": "2020-05-13T23:42:20.372111Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to add newly scraped listings on top ensuring no duplicates\n",
    "def add_listings(df, df_new):\n",
    "    cutoff = df.iloc[0].Date  #only listings with dates after most recent will be added\n",
    "    df = pd.concat([df_new[df_new.Date > cutoff], df])\n",
    "    df.Date = pd.to_datetime(df.Date)\n",
    "    return df.sort_values('Date', ascending=False).reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T00:08:16.975239Z",
     "start_time": "2020-05-15T00:08:16.884886Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add lists to dataframe\n",
    "df['Location'] = location\n",
    "df['Seller'] = seller\n",
    "df['Feedback'] = feedback\n",
    "df['FBScore'] = fbscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we'll clean up the values in the `Shipping` column.\n",
    "> Note that due to the prevalence of [] values (not as a string but an empty set) the range of values had to be explored manually because the .value_counts() function would throw an error.\n",
    "\n",
    "The range of values included: Free Shipping, +$xx.xx (x=integer), [] (local pickup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T17:21:40.914645Z",
     "start_time": "2020-05-14T17:21:40.888287Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to convert all 'Shipping' values to float\n",
    "def clean_shipping(df):\n",
    "    i = 0  #index counter\n",
    "    for ship in df.Shipping:\n",
    "        ship = str(ship)  #to avoid errors due to [] values\n",
    "        if ship[0].lower() == 'f':  #Free shipping value\n",
    "            df.Shipping.iloc[i] = 0\n",
    "        elif ship[0]=='+':  #float value starting with +$ (eg. '+$15.00 shipping')\n",
    "            df.Shipping.iloc[i] = ship[2:].split(' ')[0]\n",
    "        else:  #all else are local pickup\n",
    "            df.Shipping.iloc[i] = 0\n",
    "        i+=1\n",
    "    df.Shipping = df.Shipping.astype('float64')\n",
    "    # Also convert Price, Feedback, FBScore columns to float\n",
    "    df.Price = df.Price.astype('float64')\n",
    "    df.Feedback = df.Feedback.astype('float64')\n",
    "    df.FBScore = df.FBScore.astype('float64')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T00:08:54.088534Z",
     "start_time": "2020-05-15T00:08:53.966447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 232 entries, 0 to 232\n",
      "Data columns (total 10 columns):\n",
      "Date         232 non-null datetime64[ns]\n",
      "Title        232 non-null object\n",
      "Price        232 non-null float64\n",
      "Shipping     232 non-null float64\n",
      "Condition    232 non-null object\n",
      "Link         232 non-null object\n",
      "Location     232 non-null object\n",
      "Seller       232 non-null object\n",
      "Feedback     232 non-null float64\n",
      "FBScore      232 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(4), object(5)\n",
      "memory usage: 19.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df = clean_shipping(df)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For `Condition`, we have the eBay provided values: Brand New / Pre-Owned / Refurbished / Open Box / Parts Only / New (Other). Some users created their own values (seems by mistake intended for other fields) and upon review they can be lumped into New (Other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T17:23:57.557894Z",
     "start_time": "2020-05-14T17:23:57.540358Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to standardize all 'Condition' values\n",
    "def clean_condition(df):\n",
    "    cond_list = ['Brand New', 'Pre-Owned', 'Refurbished', 'Open Box', 'Parts Only', 'New (Other)']\n",
    "    # Filter for rows with values not in list and change to 'New (Other)'\n",
    "    i = df[df.Condition.isin(cond_list)==False].index\n",
    "    df.Condition.loc[i] = 'New (Other)'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T00:09:08.949155Z",
     "start_time": "2020-05-15T00:09:08.909229Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pre-Owned      137\n",
       "Brand New       66\n",
       "Open Box        18\n",
       "Parts Only       6\n",
       "New (Other)      4\n",
       "Refurbished      1\n",
       "Name: Condition, dtype: int64"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = clean_condition(df)\n",
    "df.Condition.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To make EDA easier, let's modify the `Location` values into a list (City, State, Country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T17:25:05.851265Z",
     "start_time": "2020-05-14T17:25:05.839304Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to convert Location values into list\n",
    "def loc_list(df):\n",
    "    i = 0  #index counter\n",
    "    for x in df.Location:\n",
    "        if len(x.split(','))==2:  #filter for missing state/district, fill in 'STATE'\n",
    "            df['Location'].iloc[i] = [x.split(',')[0], 'STATE', x.split(',')[1].lstrip()]\n",
    "        else:\n",
    "            df['Location'].iloc[i] = [x.split(',')[0], x.split(',')[1].lstrip(), x.split(',')[2].lstrip()]\n",
    "        i+=1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T00:09:24.896046Z",
     "start_time": "2020-05-15T00:09:24.688680Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Shipping</th>\n",
       "      <th>Condition</th>\n",
       "      <th>Link</th>\n",
       "      <th>Location</th>\n",
       "      <th>Seller</th>\n",
       "      <th>Feedback</th>\n",
       "      <th>FBScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2020-02-24 22:20:00</td>\n",
       "      <td>Nintendo Switch HAC-001(-01) 32GB Console  BUNDLE MUST LOOK !</td>\n",
       "      <td>710.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Pre-Owned</td>\n",
       "      <td>https://www.ebay.com/itm/Nintendo-Switch-HAC-001-01-32GB-Console-BUNDLE-MUST-LOOK/174192016632?h...</td>\n",
       "      <td>[Oceanside, California, United States]</td>\n",
       "      <td>arvaldezp_0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>96.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-24 21:11:00</td>\n",
       "      <td>Nintendo Switch Animal Crossing: New Horizon Special Edition Console Preorder</td>\n",
       "      <td>449.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Brand New</td>\n",
       "      <td>https://www.ebay.com/itm/Nintendo-Switch-Animal-Crossing-New-Horizon-Special-Edition-Console-Pre...</td>\n",
       "      <td>[San Francisco, California, United States]</td>\n",
       "      <td>astrosparky</td>\n",
       "      <td>122.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2020-02-24 21:10:00</td>\n",
       "      <td>Nintendo Switch Console (Joy-Cons) + Carry Case</td>\n",
       "      <td>249.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Pre-Owned</td>\n",
       "      <td>https://www.ebay.com/itm/Nintendo-Switch-Console-Joy-Cons-Carry-Case-/333526212695?_trksid=p2349...</td>\n",
       "      <td>[Lincoln Park, New Jersey, United States]</td>\n",
       "      <td>twobrothersgames</td>\n",
       "      <td>27761.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Date  \\\n",
       "0 2020-02-24 22:20:00   \n",
       "1 2020-02-24 21:11:00   \n",
       "2 2020-02-24 21:10:00   \n",
       "\n",
       "                                                                           Title  \\\n",
       "0                  Nintendo Switch HAC-001(-01) 32GB Console  BUNDLE MUST LOOK !   \n",
       "1  Nintendo Switch Animal Crossing: New Horizon Special Edition Console Preorder   \n",
       "2                                Nintendo Switch Console (Joy-Cons) + Carry Case   \n",
       "\n",
       "    Price  Shipping  Condition  \\\n",
       "0  710.00       0.0  Pre-Owned   \n",
       "1  449.99       0.0  Brand New   \n",
       "2  249.99       0.0  Pre-Owned   \n",
       "\n",
       "                                                                                                  Link  \\\n",
       "0  https://www.ebay.com/itm/Nintendo-Switch-HAC-001-01-32GB-Console-BUNDLE-MUST-LOOK/174192016632?h...   \n",
       "1  https://www.ebay.com/itm/Nintendo-Switch-Animal-Crossing-New-Horizon-Special-Edition-Console-Pre...   \n",
       "2  https://www.ebay.com/itm/Nintendo-Switch-Console-Joy-Cons-Carry-Case-/333526212695?_trksid=p2349...   \n",
       "\n",
       "                                     Location             Seller  Feedback  \\\n",
       "0      [Oceanside, California, United States]        arvaldezp_0      90.0   \n",
       "1  [San Francisco, California, United States]        astrosparky     122.0   \n",
       "2   [Lincoln Park, New Jersey, United States]   twobrothersgames   27761.0   \n",
       "\n",
       "   FBScore  \n",
       "0     96.9  \n",
       "1    100.0  \n",
       "2    100.0  "
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = loc_list(df)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix missing location values ([], \\n) if needed\n",
    "for i in df[df.Location=='\\n'].index:\n",
    "    req = requests.get(df.Link.iloc[i])\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    listing = soup.find_all('tr', attrs={'class': 'vi-ht20'})\n",
    "    df.Location.iloc[i] = listing[4].find_all('div', attrs={'class': 'u-flL'})[1].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T00:18:25.888603Z",
     "start_time": "2020-05-15T00:18:15.257390Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fix location values incorrectly stored as string instead of list\n",
    "notlist = []\n",
    "i = 0\n",
    "for x in df.Location:\n",
    "    if not isinstance(x, list): notlist.append(i)\n",
    "    i+=1\n",
    "\n",
    "for i in notlist:\n",
    "    df.Location.iloc[i] = [df.Location.iloc[i][1:-1].split(',')[0][1:-1],df.Location.iloc[i][1:-1].split(',')[1][2:-1],\n",
    "                               df.Location.iloc[i][1:-1].split(',')[2][2:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_shipping(df)\n",
    "df = clean_condition(df)\n",
    "df = loc_list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T00:14:39.935140Z",
     "start_time": "2020-05-15T00:14:38.970178Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df_new, df_gray, df_misc, df])\n",
    "df.Date = pd.to_datetime(df.Date)\n",
    "df = df.sort_values('Date',ascending=False).reset_index().drop(columns='index')\n",
    "\n",
    "#df[df.duplicated(['Date','Title','Price','Seller','City'], keep=False)]\n",
    "#df = df.drop_duplicates(['Date','Title','Price','Seller','City'])\n",
    "#df.to_csv('df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T00:13:46.185370Z",
     "start_time": "2020-05-15T00:13:46.160126Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((232, 10), (1978, 10), (9200, 10), (3733, 10))"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, df_new.shape, df_gray.shape, df_misc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T01:03:01.961444Z",
     "start_time": "2020-05-15T01:03:01.918721Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_lite = pd.read_csv('df_lite.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's move the Switch Lites incorrectly categorized as 'main' Switch systems to our 'lite' dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T00:38:45.835306Z",
     "start_time": "2020-05-15T00:38:45.808559Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "540"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lite = []\n",
    "i = 0\n",
    "for t in df.Title:  #record all indexes with 'lite' in title\n",
    "    if 'lite' in t.lower(): lite.append(i)\n",
    "    i+=1\n",
    "print(len(lite))\n",
    "\n",
    "temp = df.iloc[lite]\n",
    "df_lite = pd.concat([df_lite, temp])  #add identified listings to lite df\n",
    "df_lite.Date = pd.to_datetime(df_lite.Date)\n",
    "df_lite = df_lite.sort_values('Date',ascending=False).reset_index(drop=True)\n",
    "df_lite = df_lite.drop_duplicates(['Date','Title','Price','Shipping','Seller','Link'])  #drop duplicates\n",
    "df.drop(temp.index,inplace=True)  #drop identified listings from main df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T01:07:19.395745Z",
     "start_time": "2020-05-15T01:07:19.383907Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T02:18:09.425855Z",
     "start_time": "2020-05-15T02:18:09.417905Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14236, 10), (1282, 10))"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, df_lite.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T01:07:52.950279Z",
     "start_time": "2020-05-15T01:07:52.906001Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pickle throwing errors - convert to csv file to save data\n",
    "df.to_csv('df.csv',index=False)\n",
    "df_lite.to_csv('df_lite.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T23:19:16.778936Z",
     "start_time": "2020-04-29T23:19:16.520587Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get listing id\n",
    "ids = []\n",
    "for url in df_gray.Link:\n",
    "    temp = url.split('?')[0]\n",
    "    ids.append(temp[-12:-1])\n",
    "\n",
    "df_gray['ids'] = ids\n",
    "df_gray.ids = df_gray.ids.apply(lambda x: 'https://www.ebay.com/p/23024812516?iid='+str(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
